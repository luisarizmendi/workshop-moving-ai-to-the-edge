{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv11 Training with Roboflow Dataset\n",
    "\n",
    "This notebook demonstrates how to train a YOLOv11 model using a dataset from Roboflow. It includes:\n",
    "- Automatic GPU/CPU detection\n",
    "- Configurable training parameters\n",
    "- Training visualization and analysis\n",
    "\n",
    "## Step 1: Install Dependencies\n",
    "First, we'll install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVED -> Using custom container image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries\n",
    "Import all necessary libraries for training and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common\n",
    "import  os\n",
    "\n",
    "# For Dataset manipulation\n",
    "import yaml\n",
    "from roboflow import Roboflow\n",
    "\n",
    "# For training\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download Dataset from Roboflow\n",
    "Connect to Roboflow and download the dataset. Make sure to use your own API key and project details.\n",
    "\n",
    "**Remember to replace the placeholders with your values**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = Roboflow(api_key=os.getenv(\"ROBOFLOW_KEY\"))  \n",
    "\n",
    "project = rf.workspace(os.getenv(\"ROBOFLOW_WORKSPACE\")).project(os.getenv(\"ROBOFLOW_PROJECT\")) \n",
    "\n",
    "version = project.version(os.getenv(\"ROBOFLOW_DATASET_VERSION\")) \n",
    "dataset = version.download(\"yolov11\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to explicitly specify the paths to each data split (training, validation, and test) in your configuration. This ensures YOLO can correctly locate and utilize your dataset files.\n",
    "\n",
    "This is done in the `data.yaml` file. If you open that file you will see these paths that you need to update:\n",
    "\n",
    "```\n",
    "train: ../train/images\n",
    "val: ../valid/images\n",
    "test: ../test/images\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset downloaded to: {dataset.location}\")\n",
    "\n",
    "dataset_yaml_path = f\"{dataset.location}/data.yaml\"\n",
    "\n",
    "with open(dataset_yaml_path, \"r\") as file:\n",
    "    data_config = yaml.safe_load(file)\n",
    "\n",
    "data_config[\"train\"] = f\"{dataset.location}/train/images\"\n",
    "data_config[\"val\"] = f\"{dataset.location}/valid/images\"\n",
    "data_config[\"test\"] = f\"{dataset.location}/test/images\"\n",
    "\n",
    "with open(dataset_yaml_path, \"w\") as file:\n",
    "    yaml.safe_dump(data_config, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataset variable to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('dataset.pkl', 'wb') as file:\n",
    "    pickle.dump(dataset, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
