= Model Serving

The Model Serving Phase is where a validated machine learning model is prepared for production use. It ensures seamless deployment, reproducibility, and scalability while maintaining accountability.

Key aspects to take into account while AI Model releasing:

* *Versioning and Registry*: The model is versioned in a registry with metadata like training data, performance metrics, and hyperparameters. This step was already done by the Kubeflow Pipeline by publishing the model into the OpenShift AI Model Registry.
* *Packaging*: The model and its dependencies are prepared for consistent execution by containerizing them or converting them into standardized formats like ONNX or TorchScript. During the workshop, we demonstrated how to "export" the model from its native PyTorch format into other formats. For instance, TorchScript is particularly well-suited for edge deployments due to its compact size and efficiency. However, for simplicity in this workshop, you will continue using the native PyTorch file format. This approach ensures flexibility while allowing you to explore the practical benefits of exporting models in production scenarios.
* *Validation*: Additional tests, such as performance, integration, and scalability, are conducted in staging environments to ensure the model meets production requirements. In this workshop, we will perform a simple live test of the model to verify its functionality before proceeding with full deployment to edge devices.
* *Deployment*: Techniques like blue-green or canary deployments are commonly used to ensure a smooth and reliable transition to production. While OpenShift AI offers Model Serving capabilities through `KServe` and `ModelMesh` for models within the same cluster, deploying inference on a different environment—such as a Single Node OpenShift cluster or directly on Red Hat Enterprise Linux—requires a separate workflow to handle model transfer and deployment. In this workshop, we demonstrate the use of a custom Inference Server to host the PyTorch model, tailored for deployment beyond the primary OpenShift AI cluster.
* *Monitoring*: Metrics such as latency, resource usage, and data drift are continuously tracked to ensure the model's performance remains optimal. However, when deploying models at the Edge, the monitoring tools provided by OpenShift AI (e.g., TrustyAI, Grafana) cannot be used directly. Alternative monitoring strategies suited for Edge environments will be explored in detail in the next section.

In this section, we will explore how to deploy our model on a custom inference server and examine the overall application architecture, which is built on microservices. This architecture leverages the model's predictions to create a system that raises alarms when individuals are not wearing hardhats. Additionally, we will demonstrate how to conduct a quick live test of the trained model to ensure it functions as expected.

image::ai-deploy-nav0.png[]

=== Tools and preparations

In this step, you will build several container images locally. To do this, ensure you have either `docker` or `podman` installed on your laptop. Additionally, you will need access to a Container Image Registry to publish these images.

[NOTE]

The examples below use https//quay.io[Quay.io] as the Container Registry.



== Serving

image::ai-deploy-nav1.png[]

OpenShift AI provides robust serving platforms to deploy models as inference endpoints:

* Single-model serving (https://github.com/kserve/kserve[`KServe`]): Designed for large models requiring dedicated resources.

* Multi-model serving (https://github.com/kserve/modelmesh[`ModelMesh`]): Optimized for small and medium-sized models sharing resources.

As mentioned, these options are not available for our use case since the deployment will occur at the Edge, remember that for our use case the MLOps workflow diagram is:

image::ai-build-rhoai-features.png[]

When deploying on Red Hat Enterprise Linux, there are several options for Model Serving. However, to simplify things in this workshop, we have created a custom Python-based inference server.


Although this part is more typically the responsibility of Application Developers rather than AI Specialists, for the purposes of this workshop we will briefly introduce the applications involved in the use case. Since the workshop doesn't cover the full application development lifecycle, this overview will help you understand how the AI model fits into the broader system.



=== Overview of the solution



Before diving into the deployment details, let’s first understand the overall solution architecture, including the microservices involved and how they communicate. In this architecture we use a webcam to detect objects at the edge, and how those detections can trigger messages/alarms that can be visualized in a dashboard on the Core Datacenter/Cloud.


image::ai-deploy-object-detection-webcam.png[]

The solution is based on the following microservices, you can clik on the names to get detailed information about each one:

* https://github.com/luisarizmendi/workshop-moving-ai-to-the-edge/blob/main/resources/solutions/ai-specialist/serving/apps/object-detection-stream-manager[Camera Stream Manager]

* https://github.com/luisarizmendi/workshop-moving-ai-to-the-edge/blob/main/resources/solutions/ai-specialist/serving/apps/object-detection-inference-server[Inference server]

* https://github.com/luisarizmendi/workshop-moving-ai-to-the-edge/blob/main/resources/solutions/ai-specialist/serving/apps/object-detection-action[Actuator service]

* https://github.com/luisarizmendi/workshop-moving-ai-to-the-edge/blob/main/resources/solutions/ai-specialist/serving/apps/object-detection-dashboard/src/backend[Dashboard backend]

* https://github.com/luisarizmendi/workshop-moving-ai-to-the-edge/blob/main/resources/solutions/ai-specialist/serving/apps/object-detection-dashboard/src/frontend[Dashboard frontend]


The workflow is the following:

1. The Camera Stream Manager sends images to the Inference API
2. The Inference Server, that contains the AI model detecting objects, returns the predictions
3. The "Action" service calls the inference endpoint and if detects certain objects it will trigger an alarm, that is sent to the database hosted in a remote site.
4. The information of the device is shown in the Dashboard



Although application development is not covered in this workshop, it plays a fundamental role and can also be carried out on OpenShift. You can deploy on top of OpenShift several tools to support application development, such as:

* *OpenShift Developer Hub*: This internal developer portal centralizes access to resources, templates, and documentation, accelerating the onboarding process and standardizing application development across teams. It ensures consistency and visibility into microservices and APIs.

* *OpenShift Dev Spaces*: OpenShift Dev Spaces provides cloud-based development environments accessible directly from the browser. It offers pre-configured, containerized workspaces that mirror production, ensuring developers can write, test, and debug code in an environment that reflects the final deployment conditions.

* *OpenShift Pipelines*: Built on Tekton, OpenShift Pipelines automates CI/CD workflows, enabling fast, consistent builds, tests, and deployments of containerized applications. This tool ensures rapid iteration and integration, reducing downtime and accelerating feature delivery.

* *Quay Container Image Registry*: A secure container image registry stores and manages application images, ensuring that developers can reliably push, pull, and deploy containers to different environments. It supports versioning and helps enforce security and compliance policies.

* *OpenShift GitOps*: Implements GitOps practices for application deployment and lifecycle management.


All of these tools can be utilized throughout the application development lifecycle, which mirrors the MLOps cycle we are following in this workshop. Both cycles share common stages, such as:

image::dev-workflow.png[]

1. *App Planning*: Architecture Design: This foundational phase focuses on system design decisions, technology stack selection, and establishing the technical approach. It sets the blueprint for the entire application development lifecycle.

2. *App Development*: This encompasses the core development activities:

    * Code Development: Writing application code following established design patterns and best practices. This involves implementing features and functionality according to requirements.
    * Testing: Comprehensive testing. This phase often requires iteration back to code development to address identified issues.

3. *App Release*: Integration, Deployment: After successful testing, the application is prepared for production, involving integration with other systems and services, deployment through CI/CD pipelines,final verification in staging environments and production rollout

4. *Day-2 Operations*: Monitoring, Tuning: Post-deployment activities focus on application performance monitoring, resource utilization optimization and performance tuning based on real world usage




=== Custom Inference Server

The custom inference server is a FastAPI application that provides an object detection system using a built-in model. It offers the following RESTful API endpoints:

1. `/v1/models/{model_name}/infer` (POST)
- **Purpose**: Make predictions on an image
- **Request Body**: JSON with base64 encoded image and optional confidence threshold
- **Returns**: JSON with detections, inference time, and metadata

2. `/v1/models/{model_name}` (GET)
- **Purpose**: Get model status information
- **Returns**: JSON with model name, ready status, load time, and device

3. `/v1/models/{model_name}/load` (POST)
- **Purpose**: Load a model
- **Parameters**: model_name and model_path
- **Returns**: Success/failure message

4. `/healthz` (GET)
- **Purpose**: Health check endpoint
- **Returns**: System health status, GPU availability, and model status


It's also important to mention that the https://github.com/luisarizmendi/workshop-moving-ai-to-the-edge/blob/main/resources/solutions/ai-specialist/serving/apps/object-detection-inference-server/src/Containerfile[container image] was created embedding the model directly into it, meaning that the model versioning is tied to the container image versioning.


Once it is deployed, you can test it by using `curl`, for example you can check the Inference Server health:

----
curl http://<inference url>:<inference port>/healthz
----

The answer will provide valuable information about whether GPU inferencing is available and confirm whether the model has been successfully loaded and is ready for inference:

----
{
    "status": "healthy",
    "gpu_available": false,
    "model_loaded": false,
    "model_name": null,
    "timestamp": "2024-01-28T14:30:25.123456"
}
----


You can also run object detection using a local image file (`image.jpg` infered by the model named `default` in the following example): 

----
curl -X POST http://<inference url>:<inference port>/v1/models/default/infer \
  -H "Content-Type: application/json" \
  -d '{
    "image": "'$(base64 -w 0 image.jpg)'",
    "confidence_threshold": 0.25
  }'
----

The response will include details about any detections made, such as the class name, the confidence score of the detection, and the coordinates of the detection bounding box.

----
{
    "detections": [
        {
            "class_name": "hardhat",
            "confidence": 0.72,
            "bbox": [100.0, 200.0, 300.0, 400.0]
        }
    ],
    "inference_time": 0.156,
    "model_name": "default",
    "timestamp": "2024-01-27T10:30:45.123456"
}
----



=== Building the Microservices

[TIP]
====
If you don't want to create the images on your own you can use the following:

* https://quay.io/repository/luisarizmendi/object-detection-stream-manager?tab=tags[Camera Stream Manager container image]: quay.io/luisarizmendi/object-detection-stream-manager:prod
* https://quay.io/repository/luisarizmendi/object-detection-inference-server?tab=tags[Inference server container image]: quay.io/luisarizmendi/object-detection-inference-server:prod
* https://quay.io/repository/luisarizmendi/object-detection-action?tab=tags[Actuator service container image]: quay.io/luisarizmendi/object-detection-action:prod
* https://quay.io/repository/luisarizmendi/object-detection-dashboard-backend?tab=tags[Dashboard backend container image]: quay.io/luisarizmendi/object-detection-dashboard-backend:prod
* https://quay.io/repository/luisarizmendi/object-detection-dashboard-frontend?tab=tags[Dashboard frontend container image]: quay.io/ luisarizmendi/object-detection-dashboard-frontend:prod
====

When planning to create a container image, it's essential to consider the system architecture of the device where the application will run. For deployments on Public Cloud or Core Data Centers, this is usually straightforward since the architecture will likely be x86. However, for Edge use cases, the situation is different.

In our case, we will build x86 container images for the services that are running on the Cloud and container images for both x86 and ARM architectures for the ones running at the Edge, leveraging the multi-architecture container image feature available in certain Container Image Registries like Quay.

You can find the Containerfiles for each application in the https://github.com/luisarizmendi/workshop-moving-ai-to-the-edge/tree/main/resources/solutions/ai-specialist/serving/apps[`src` directory of each application's URL] shared above, along with the application code and other associated files. Feel free to clone the repository and use these files for your own builds.

[NOTE]

The build process is closely tied to the architecture of the system you're using. If you build on an ARM-based system, the resulting container images will default to the ARM architecture unless you specifically perform cross-compilation (which we'll cover in more detail below). For this workshop, we will be using an x86-based system for the build process.

For this workshop, we will be building the container images locally instead of utilizing an automated CI/CD pipeline. For the applications running on the Cloud (x86 only) you just need to build the images in the normal way:

----
cd <application src directory>
podman build -t <registry>/<namespace>/<container image name>:<tag> .
podman login -u <user> <registry>
podman push <registry>/<namespace>/<container image name>:<tag> 
----

[NOTE]

Remember to make the images public once you pushed into the registry for simplicity, otherwise you will need to configure the registry credentials in order to pull the images.




For the applications that will run at the Edge, we need to create container images for both x86 and ARM architectures. If you're working on an x86 system, you can build the x86 images using the same procedure as outlined earlier. But what about the ARM images? Strictly speaking, you would need access to an ARM system to build ARM-based images. However, you can use cross-compilation tools to create ARM images directly on x86 systems (and vice versa). In this section, we’ll focus on how to perform cross-compilation on x86 RHEL-based systems.

If your laptop is running a Fedora/RHEL based Linux distribution, you can leverage `qemu-user-static` to enable the creation of ARM images (In MacOS, even with M3 chip, you can direcly `brew` podman with the `--platform` option as shown below, there is no need to install the `qemu-user-static` package)

What is qemu-user-static? `qemu-user-static` is a versatile user-space emulator that enables programs compiled for one architecture to run on another. By using this tool, you can emulate the ARM architecture on an x86 system, making it possible to build and test ARM container images without requiring native ARM hardware.


[example]
====
Before building the ARM container images, follow these steps to set up qemu-user-static:

1- Install the `qemu-user-static` package in your system 

[source,shell,role=execute,subs="attributes"]
----
sudo dnf install podman qemu-user-static
----

2- Run the `qemu-user-static` container  to enable multi-architecture support

[source,shell,role=execute,subs="attributes"]
----
sudo podman run --rm --privileged multiarch/qemu-user-static --reset -p yes
----
====

With these steps completed, you can proceed to build ARM-based container images on your x86 system as if you were working on native ARM hardware.

[example]
====
In order to create the create the ARM image you will need to run the `podman build` but including the desired ARM architecture with the argument `--platform linux/arm64`.

----
podman build --platform linux/arm64  -t <arm image>:<tag> .
----
====

[IMPORTANT]

From now on, it's recommended to always specify the platform explicitly when building container images (e.g., `linux/arm64` for ARM and `linux/amd64` for x86...yes, the naming is quite similar). This is because Podman "remembers" the last platform you used during the build process, and without explicitly setting the platform, you might unintentionally create an ARM image when you only intended to build a standard x86 image. Explicitly defining the platform ensures clarity and prevents unexpected results.

[TIP]

It's a good idea to mention in the image `tag` if that image is intended to be used in an x86 or ARM system (e,g, `<image>:arm-v1`)

At this point you will have the https://github.com/luisarizmendi/workshop-moving-ai-to-the-edge/blob/main/resources/solutions/ai-specialist/serving/apps/object-detection-stream-manager[Camera Stream Manager], https://github.com/luisarizmendi/workshop-moving-ai-to-the-edge/blob/main/resources/solutions/ai-specialist/serving/apps/object-detection-inference-server[Inference server] and https://github.com/luisarizmendi/workshop-moving-ai-to-the-edge/blob/main/resources/solutions/ai-specialist/serving/apps/object-detection-action[Actuator service] images for both ARM and x86 systems but you had to use different tags, otherwise you will be "overwriting" one image with other, this is not ideal since the image image name will be different depending on the system that you are deploying to.

But that's not a big problem since the multi-arch container images exist.

You can create a "pseudo container image" called a manifest that references multiple images for different architectures. At deployment time, the runtime automatically selects the appropriate image based on the system architecture. This allows you to use the same container image name and tag consistently, regardless of the system on which it is deployed.


[example]
====
Let's create a multi-arch container image and push it into our registry.

1- Create the manifest

----
podman manifest create <registry>/<namespace>/<container image name>:<shared tag>
----

2- Add the images that you created for both architectures 

----
podman manifest add <registry>/<namespace>/<container image name>:<shared tag> <registry>/<namespace>/<container image name>:<x86 tag>
podman manifest add <registry>/<namespace>/<container image name>:<shared tag> <registry>/<namespace>/<container image name>:<arm tag>
----

3- Push the manifest. Pay attention that the command is `podman manifest push`, not just `podman push`

----
podman manifest push <registry>/<namespace>/<container image name>:<shared tag> 
----
====

Now you can use the `<registry>/<namespace>/<container image name>:<shared tag>` to deploy the container seamlessly on both x86 and ARM systems, with the runtime automatically selecting the correct architecture-specific image.















== End-to-End Validation

image::ai-deploy-nav2.png[]


So far, you have completed the AI Specialist's tasks by creating the model and taken on some Application Development responsibilities by manually building the container images. Now, before handing over to the Platform Specialist for deploying the applications to the Edge devices, it's a good idea to perform a final test of the model you created. Let’s deploy all the components together and verify if everything works as expected.

For this test, we will use your own laptop as the "Edge Device" and deploy the cloud-side applications in a temporary OpenShift project, which can easily be deleted after the testing is complete.



=== Cloud-side Applications deployment


[NOTE]

Instructions below are using the provided pre-created container images, but you can use your own images if you created them in the previous step.


[example]
====
Follow the steps below to create the https://github.com/luisarizmendi/workshop-moving-ai-to-the-edge/blob/main/resources/solutions/ai-specialist/serving/apps/object-detection-dashboard/src/backend[Dashboard backend] and https://github.com/luisarizmendi/workshop-moving-ai-to-the-edge/blob/main/resources/solutions/ai-specialist/serving/apps/object-detection-dashboard/src/frontend[Dashboard frontend] applications in OpenShift:


1- Navegate to the "Administrator" view in the https://console-openshift-console.apps.{ocp_cluster_url}[OpenShift Console]

2- Log in using your OpenShift credentials: {openshift-user}  /  {openshift-password}.

3- Create a new OpenShift Project (`{user}--test`)

4- Click on the `+` icon on the top right corner of the OpenShift console.

5- Paste there the content shown below to deploy the Dashboard Backend and Click "Create".

[source,yaml,role=execute,subs="attributes"]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: object-detection-dashboard-backend
  labels:
    app: object-detection-dashboard
    app.kubernetes.io/part-of: Dashboard
    app.openshift.io/runtime: "python"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: object-detection-dashboard
      component: backend
  template:
    metadata:
      labels:
        app: object-detection-dashboard
        component: backend
    spec:
      containers:
      - name: backend
        image: quay.io/luisarizmendi/object-detection-dashboard-backend:v1
        ports:
        - containerPort: 5005
---
apiVersion: v1
kind: Service
metadata:
  name: object-detection-dashboard-backend
  labels:
    app: object-detection-dashboard
spec:
  selector:
    app: object-detection-dashboard
    component: backend
  ports:
  - protocol: TCP
    port: 5005
    targetPort: 5005
  type: ClusterIP
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: object-detection-dashboard-backend
  labels:
    app: object-detection-dashboard
spec:
  to:
    kind: Service
    name: object-detection-dashboard-backend
  port:
    targetPort: 5005
----

6- Open the "Route" object and take note of the Dashboard Backend URL, you will need it.

7- Click again on the `+` icon on the top right corner of the OpenShift console. Copy the code below and paste it there, **but before creating the object** include in the placeholder `HERE-YOU-BACKEND-API-BASE-URL---` the Dashboard Backend URL that you copied in the previous step.


----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: object-detection-dashboard-frontend
  labels:
    app: object-detection-dashboard
    app.kubernetes.io/part-of: Dashboard
    app.openshift.io/runtime: "nodejs"
  annotations:
    app.openshift.io/connects-to: '[{"apiVersion":"apps/v1","kind":"Deployment","name":"object-detection-dashboard-backend"}]'
spec:
  replicas: 1
  selector:
    matchLabels:
      app: object-detection-dashboard
      component: frontend
  template:
    metadata:
      labels:
        app: object-detection-dashboard
        component: frontend
    spec:
      containers:
      - name: frontend
        image: quay.io/luisarizmendi/object-detection-dashboard-frontend:v1
        ports:
        - containerPort: 3000
        env:
        - name: BACKEND_API_BASE_URL
          value: HERE-YOU-BACKEND-API-BASE-URL-!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!-DONT-FORGET-TO-COMPLETE
---
apiVersion: v1
kind: Service
metadata:
  name: object-detection-dashboard-frontend
  labels:
    app: object-detection-dashboard
spec:
  selector:
    app: object-detection-dashboard
    component: frontend
  ports:
  - protocol: TCP
    port: 3000
    targetPort: 3000
  type: ClusterIP
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: object-detection-dashboard-frontend
  labels:
    app: object-detection-dashboard
spec:
  to:
    kind: Service
    name: object-detection-dashboard-frontend
  port:
    targetPort: 3000
----

6- Open the "Route" object and take note of the Dashboard Frontend URL.

====


When all pods are running, you will be able to open the Dashboard using the Frontend URL. You will see an empty page with the "Device Monitoring Dashboard" title.

[CAUTION]

The Dashboard application does not use TLS, so the URL must start `http://` and `https://` otherwhile you will get a message "Application is not available" even when then POD is already running.


=== Local machine applications deployment

You’ve successfully deployed the cloud-side applications! Now, take the next step by running the remaining applications on your own laptop

[NOTE]

Instructions for Fedora/RHEL based systems and using the interactive mode, so you can review live logs easily (you will need to use three different command line terminals).

[CAUTION]

Be sure that you have the ports `tcp/8080` and `tcp/5000` ports open un your local machine. 

[example]
====

1- Deploy the Inference Server:

[source,shell,role=execute,subs="attributes"]
----
podman run -it --rm -p 8080:8080 quay.io/luisarizmendi/object-detection-inference-server:prod
----

[NOTE]

This is a large image, the pull could take time

If you have an https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/cdi-support.html[NVIDA GPU and you have it configured in your system] (`sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml`), you might want to use it for inferencing by running `podman run -it --rm -p 8080:8080 --device nvidia.com/gpu=all --security-opt=label=disable quay.io/luisarizmendi/object-detection-inference-server:prod`


2- Now you can check that the GPU is being detected by checking the `healthz` endpoint, see an example below.

----
curl http://localhost:8080/healthz
{"status":"healthy","gpu_available":true,"model_loaded":true,"model_name":"1","timestamp":"2025-01-28T22:34:31.102136"}
----

3- Deploy the Camera stream manager. In this case you will need to run it as privileged to access the system devices (webcams) and also to use the host network, so it can reach out to the inference server.

[source,shell,role=execute,subs="attributes"]
----
sudo podman run -it --rm -p 5000:5000 --privileged --network=host quay.io/luisarizmendi/object-detection-stream-manager:prod
----

[NOTE]

This won't work for MAC users since camera access from containers is different in that OS. If you are a MAC user you can still run this service by downloading and running directly the https://github.com/luisarizmendi/workshop-moving-ai-to-the-edge/blob/main/resources/solutions/ai-specialist/serving/apps/object-detection-stream-manager/src/object-detection-stream-manager.py[`object-detection-stream-manager.py` python script] on your system with `python object-detection-stream-manager.py`. Remember to install the https://github.com/luisarizmendi/workshop-moving-ai-to-the-edge/blob/main/resources/solutions/ai-specialist/serving/apps/object-detection-stream-manager/src/requirements.txt[python dependencies] with `pip install` and to grant permissions to access cameras if needed.



4- Deploy the Actuator. It needs also to use the host network. Also you will need to include the Dashboard backend route. Please, don't forget the `/alert` and `/alive` as part of the environment variable value.

----
podman run -it --rm --network=host -e ALERT_ENDPOINT=<DASHBOARD_BACKEND_OCP_ROUTE>/alert -e ALIVE_ENDPOINT=<DASHBOARD_BACKEND_OCP_ROUTE>/alive quay.io/luisarizmendi/object-detection-action:prod
----

====


[NOTE]

You will see logs directly in the console terminal. The Inference Server may already be displaying detection results, and if a `no_helmet` detection occurs, the "actuator" will trigger an alarm. Additionally, the Camera Stream Manager logs important details at startup, such as the selected webcam ID, providing useful insights for debugging and verification


==== The Convenient Yet Non-Edge Test Architecture

Inferencing with a CPU can be slow. However, if you have an NVIDIA GPU in your laptop, you can deploy the Inference Server using it, as demonstrated in the previous step. But what if you don’t have one?

You have two options:

Stick to the Edge Computing setup, accepting slower frame detection. While not ideal, my tests have shown it remains functional.

Leverage an OpenShift Cluster with GPUs (if available through this workshop). This allows you to deploy the Inference Server in the Cloud instead of your local machine for testing.

While the second option provides faster inferencing, it is not a true Edge Computing architecture. [.underline]*Sending images from the edge to the Cloud introduces network delays and additional costs*, two key drawbacks that edge computing is designed to mitigate. However, for a quick test, it offers a practical way to achieve high-speed inferencing.

[NOTE]

This deployment is also useful for comparing Edge vs. Non-Edge setups. You can test with or without a GPU in both environments to evaluate user experience, delays, and performance trade-offs.

Now, let’s examine the [.underline]*non-edge architecture* we’ll set up for testing. As you can see, the difference comparing it with the "pure" edge deployment is where the Inference Server is located.


image::ai-deploy-noy-edge-arch.png[]

[example]
====
Let's deploy this architecture:

1- Start by the Inference Server. In this case we will need the Kubernetes manifests that we will apply in OpenShift (you can use the project that you created, `{user}--test`):


[source,yaml,role=execute,subs="attributes"]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inference-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: inference-server
  template:
    metadata:
      labels:
        app: inference-server
    spec:
      containers:
        - name: inference-server
          image: quay.io/luisarizmendi/object-detection-inference-server:prod
          ports:
            - containerPort: 8080
          resources:
            limits:
              nvidia.com/gpu: 1 
---
apiVersion: v1
kind: Service
metadata:
  name: inference-server
spec:
  selector:
    app: inference-server
  ports:
  - protocol: TCP
    port: 8080
    targetPort: 8080
  type: ClusterIP

---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: inference-server
  labels:
    app: inference-server
spec:
  to:
    kind: Service
    name: inference-server
  port:
    targetPort: 8080
----


2- Wait until the POD is in `Running` state.

3- Go to routes and take note of the Inference Server route. Now you can check that the GPU is being detected by checking the `healthz` endpoint, see an example below.

----
curl http://inference-server-user99-test.apps.cluster-2cndn.2cndn.sandbox73.opentlc.com/healthz
{"status":"healthy","gpu_available":true,"model_loaded":true,"model_name":"1","timestamp":"2025-01-28T22:34:31.102136"}
----

4- You still need to deploy the other services locally (although you could also potentially deploy the Actuator in the cloud too), but there is a difference on how you deploy the Camera Stream Manager, since you need to indicate the URL endpoint for the Inference Server (by default it uses `localhost`). You can do it by using the `INFERENCE_SERVER_URL` environment variable (it will be something similar to `http://inference-server-user99-test.apps.cluster-2cndn.2cndn.sandbox73.opentlc.com`).

----
sudo podman run -it --rm -p 5000:5000 --privileged -e INFERENCE_SERVER_URL=<INFERENCE_SERVER_OCP_ROUTE> quay.io/luisarizmendi/object-detection-stream-manager:prod
----

5- Deploy the Actuator. 

----
podman run -it --rm --network=host -e ALERT_ENDPOINT=<DASHBOARD_BACKEND_OCP_ROUTE>/alert -e ALIVE_ENDPOINT=<DASHBOARD_BACKEND_OCP_ROUTE>/alive quay.io/luisarizmendi/object-detection-action:prod
----
====





=== Testing workflow

As part of the workshop materials, hardhats should be provided. If you don’t have one, you can use a cycling helmet, though this may reduce detection accuracy.

[NOTE]

For this initial test, you will start without wearing a hardhat.



[example]
====
Once all services are up and running, follow these steps to validate the system:


1- Open `http://localhost:5000/video_stream`. You should see the camera feed displaying a `no_helmet` detection.


image::ai-deploy-screenshot_video_stream.png[]


2- Open the Dashboard Frontend URL. If the camera has already detected anything (`helmet` or `no_helmet`), you will see a device listed with your MAC address as the Device Name.


3- Since the camera is detecting no_helmet, an alarm icon will appear next to your device name.

image::ai-deploy-screenshot_dashboard_main.png[]


4- Put on the hardhat and observe how the system detects it in the video stream. After a few seconds, the alarm should disappear.

5- Click on your Device Name to view detailed information, including logged alarms. You can also rename the device to give it a more user-friendly name.

image::ai-deploy-screenshot_dashboard_detail.png[]
====




== Solution and Next Steps

In this step, you have completed key application development tasks, including building container images for the required applications. The code and corresponding Container files for each application can be found in the https://github.com/luisarizmendi/workshop-moving-ai-to-the-edge/tree/main/resources/solutions/ai-specialist/serving/apps[`serving/apps` directory of the `ai-specialist` solution resources].


At this stage, you are well-positioned to hand over the solution to the xref:platform-specialist-00-intro.adoc[Platform Specialist] for deployment on Edge Devices. However, if you prefer to skip that step or have already completed it in a previous part of the workshop, you can proceed to the final task for the AI Specialist: the xref:ai-specialist-05-update.adoc[Day-2 Operations] section.

Do not remove the services that you deployed for your model testing since you will need it in the next section.
