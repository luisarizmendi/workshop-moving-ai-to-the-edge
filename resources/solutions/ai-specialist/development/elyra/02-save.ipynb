{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Store the Model\n",
    "\n",
    "Save the trained model to the Object Storage system configured in your Workbench connection. \n",
    "\n",
    "Start by getting the credentials and configuring variables for accessing Object Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deserialize from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "results_train_save_path = \"model_train_results.pth\"\n",
    "results_train = torch.load(results_train_save_path)\n",
    "\n",
    "results_test_save_path = \"model_test_results.pth\"\n",
    "results_test = torch.load(results_test_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_S3_ENDPOINT = os.getenv(\"S3_ENDPOINT\", \"\").replace('https://', '').replace('http://', '')\n",
    "AWS_ACCESS_KEY_ID = os.getenv(\"ACCESS_KEY\")\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"SECRET__KEY\")\n",
    "AWS_S3_BUCKET = os.getenv(\"S3_BUCKET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the S3 client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=f\"https://{AWS_S3_ENDPOINT}\",  \n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    verify=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select files to be uploaded (files generated while training and validating the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_train = results_train['save_dir']\n",
    "weights_path = os.path.join(model_path_train, \"weights\")\n",
    "model_path_test = results_test['save_dir']\n",
    "\n",
    "# Get file lists\n",
    "files_train = [os.path.join(model_path_train, f) for f in os.listdir(model_path_train) if os.path.isfile(os.path.join(model_path_train, f))]\n",
    "files_models = [os.path.join(weights_path, f) for f in os.listdir(weights_path) if os.path.isfile(os.path.join(weights_path, f))]\n",
    "files_test = [os.path.join(model_path_test, f) for f in os.listdir(model_path_test) if os.path.isfile(os.path.join(model_path_test, f))]\n",
    "\n",
    "directory_name = os.path.basename(model_path_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(file_path, s3_path):\n",
    "    try:\n",
    "        s3_client.upload_file(file_path, AWS_S3_BUCKET, s3_path)\n",
    "        print(f\"'{os.path.basename(file_path)}' uploaded successfully to '{s3_path}'.\")\n",
    "    except (NoCredentialsError, PartialCredentialsError) as e:\n",
    "        print(\"Credentials error: \", e)\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred: \", e)\n",
    "\n",
    "# Upload train files\n",
    "for file_path in files_train:\n",
    "    upload_file(file_path, f\"prototype/pipeline/{directory_name}/train-val/{os.path.basename(file_path)}\")\n",
    "\n",
    "# Upload model weights\n",
    "for file_path in files_models:\n",
    "    upload_file(file_path, f\"prototype/pipeline/{directory_name}/{os.path.basename(file_path)}\")\n",
    "\n",
    "# Upload test files\n",
    "for file_path in files_test:\n",
    "    upload_file(file_path, f\"prototype/pipeline/{directory_name}/test/{os.path.basename(file_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Remove local files\n",
    "\n",
    "Once you uploaded the Model data to the Object Storage, you can remove the local files to save disk space and also the files where we stored the variable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {model_path_train}\n",
    "!rm -rf {model_path_test}\n",
    "!rm -rf {results_train_save_path}\n",
    "!rm -rf {results_test_save_path}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
