apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: triton-25.01
  labels:
    opendatahub.io/dashboard: "true"
  annotations:
    opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
    openshift.io/display-name: "NVIDIA Triton Server v25.01"
spec:
  supportedModelFormats:
  # https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html
    - name: pyg
      version: "2.6.1"
      autoSelect: true
    - name: onnx
      version: "1.20.1"
      autoSelect: true
    - name: openvino
      version: "2024.05.0"
      autoSelect: true
    - name: pytorch
      version: "2.6.0"
      autoSelect: true
    - name: tensorflow
      version: "2.16.1" 
      autoSelect: true
    - name: tensorrt
      version: "10.8.0.43" 
      autoSelect: true

  multiModel: false
  protocolVersions:
    - v2
    - grpc-v2


  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "8002"

  volumes:
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: 2Gi

  containers:
    - name: kserve-container
      image: nvcr.io/nvidia/tritonserver:25.01-py3
      command: [/bin/sh]
      args:
        - -c
        - 'exec tritonserver
          "--model-repository=/mnt/models"
          "--allow-http=true"
          "--allow-sagemaker=false"
          '
      volumeMounts:
        - name: shm
          mountPath: /dev/shm
      resources:
        requests:
          cpu: 500m
          memory: 1Gi
        limits:
          cpu: "5"
          memory: 1Gi
      livenessProbe:
        # the server is listening only on 127.0.0.1, so an httpGet probe sent
        # from the kublet running on the node cannot connect to the server
        # (not even with the Host header or host field)
        # exec a curl call to have the request originate from localhost in the
        # container
        exec:
          command:
            - curl
            - --fail
            - --silent
            - --show-error
            - --max-time
            - "9"
            - http://localhost:8000/v2/health/live
        initialDelaySeconds: 5
        periodSeconds: 30
        timeoutSeconds: 10
      ports:
        - containerPort: 8000
          protocol: TCP